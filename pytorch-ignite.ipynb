{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-ignite\n",
      "  Using cached pytorch_ignite-0.4.2-py2.py3-none-any.whl (175 kB)\n",
      "Requirement already satisfied: torch<2,>=1.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from pytorch-ignite) (1.7.1)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/anaconda3/lib/python3.8/site-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/anaconda3/lib/python3.8/site-packages (from torch<2,>=1.3->pytorch-ignite) (1.19.2)\n",
      "Installing collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.utils import setup_logger\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def get_data_loaders(train_batch_size, val_batch_size):\n",
    "    data_transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        MNIST(download=True, root=\".\", transform=data_transform, train=True), batch_size=train_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        MNIST(download=False, root=\".\", transform=data_transform, train=False), batch_size=val_batch_size, shuffle=False\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def run(train_batch_size, val_batch_size, epochs, lr, momentum, log_interval):\n",
    "    train_loader, val_loader = get_data_loaders(train_batch_size, val_batch_size)\n",
    "    model = Net()\n",
    "    device = \"cpu\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    model.to(device)  # Move model before creating optimizer\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    criterion = nn.NLLLoss()\n",
    "    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "    trainer.logger = setup_logger(\"trainer\")\n",
    "\n",
    "    val_metrics = {\"accuracy\": Accuracy(), \"nll\": Loss(criterion)}\n",
    "    evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)\n",
    "    evaluator.logger = setup_logger(\"evaluator\")\n",
    "\n",
    "    pbar = tqdm(initial=0, leave=False, total=len(train_loader), desc=f\"ITERATION - loss: {0:.2f}\")\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "    def log_training_loss(engine):\n",
    "        pbar.desc = f\"ITERATION - loss: {engine.state.output:.2f}\"\n",
    "        pbar.update(log_interval)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        pbar.refresh()\n",
    "        evaluator.run(train_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_accuracy = metrics[\"accuracy\"]\n",
    "        avg_nll = metrics[\"nll\"]\n",
    "        tqdm.write(\n",
    "            f\"Training Results - Epoch: {engine.state.epoch} Avg accuracy: {avg_accuracy:.2f} Avg loss: {avg_nll:.2f}\"\n",
    "        )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(engine):\n",
    "        evaluator.run(val_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_accuracy = metrics[\"accuracy\"]\n",
    "        avg_nll = metrics[\"nll\"]\n",
    "        tqdm.write(\n",
    "            f\"Validation Results - Epoch: {engine.state.epoch} Avg accuracy: {avg_accuracy:.2f} Avg loss: {avg_nll:.2f}\"\n",
    "        )\n",
    "\n",
    "        pbar.n = pbar.last_print_n = 0\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED | Events.COMPLETED)\n",
    "    def log_time(engine):\n",
    "        tqdm.write(f\"{trainer.last_event_name.name} took { trainer.state.times[trainer.last_event_name.name]} seconds\")\n",
    "\n",
    "    trainer.run(train_loader, max_epochs=epochs)\n",
    "    pbar.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = ArgumentParser()\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=64, help=\"input batch size for training (default: 64)\")\n",
    "#     parser.add_argument(\n",
    "#         \"--val_batch_size\", type=int, default=1000, help=\"input batch size for validation (default: 1000)\"\n",
    "#     )\n",
    "#     parser.add_argument(\"--epochs\", type=int, default=10, help=\"number of epochs to train (default: 10)\")\n",
    "#     parser.add_argument(\"--lr\", type=float, default=0.01, help=\"learning rate (default: 0.01)\")\n",
    "#     parser.add_argument(\"--momentum\", type=float, default=0.5, help=\"SGD momentum (default: 0.5)\")\n",
    "#     parser.add_argument(\n",
    "#         \"--log_interval\", type=int, default=10, help=\"how many batches to wait before logging training status\"\n",
    "#     )\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     run(args.batch_size, args.val_batch_size, args.epochs, args.lr, args.momentum, args.log_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8f732537974cc88dcb26db4d2ad6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41b7c6d839942d09bb06dc529e003f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb79560b41c4f9688d913cbc71223fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da463a459df0466c877915e3cbf17362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1607370249289/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "d = {\n",
    "    'batch_size': 64,\n",
    "    'val_batch_size': 1000,\n",
    "    'epochs': 10,\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.5,\n",
    "    'log_interval': 10\n",
    "}\n",
    "args = namedtuple('Struct', d.keys())(*d.values())\n",
    "\n",
    "run(args.batch_size, args.val_batch_size, args.epochs, args.lr, args.momentum, args.log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
