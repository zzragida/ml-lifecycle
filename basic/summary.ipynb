{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 싸이언티스트 실무 관점에서:\n",
    "\n",
    "0. **문제정의**: 무엇을 분석할지 정한다\n",
    "    - 문제정의가 없으면 분석은 시작할 필요가 없다  \n",
    "        > **문제정의에 많은 고민을 해야 한다**  \n",
    "        > **문제정의에 모든 구성원이 동의할 수 있도록 끊임없이 커뮤니케이션 해야 한다**  \n",
    "        > **1회성이 문제정의가 아니라 필요시 끊임없이 진화/변경시켜야 한다**  \n",
    "\n",
    "1. **데이터수집**: 소스별 데이터 추출 및 저장(Loading)\n",
    "    - 데이터가 없으면 분석은 시작할 필요가 없다\n",
    "    - 문제 답의 보기후보가 데이터에 없으면 분석을 시작할 필요가 없다(어떤 연령이 TV를 보는지 알고 싶은데 데이터에 연령이 없으면 불가)\n",
    "        > **알고리즘/기술보다 데이터수집부터 시작하기 위한 작업을 착수해야 한다**\n",
    "        > **데이터는 많을수록 좋지만 양보다(Row) 질(Column)을 늘려야 분석을 한 의미가 생긴다**\n",
    "        > **보기가 데이터에 없으면 문제정의부터 새롭게 수정해야 한다**\n",
    "        > - Loading 목적: 각 소스별로 데이터를 수집함\n",
    "    \n",
    "2. **데이터전처리**: 기초통계(Descriptive Statistics) + 붙이기(Curation) + 없애기(Remove) + 채우기(Fill) + 필터(Filter) + 변경하기(Transform)  \n",
    "    > - Descriptive Statistics 목적: 하기 4개의 전처리 의사결정을 위한 기준으로 주로 활용\n",
    "    > - Curation 목적: 각 소스별 데이터를 하나의 Database로 붙임  \n",
    "    > - Remove & Fill 목적: 데이터 오류를 제거하가나 비어있는 데이터를 채움  \n",
    "    > - Filter 목적: 분석범위에 관련된 보기(Feature)들만을 추려냄  \n",
    "    > - Transform 목적: 사람이 이해가 가능한 방식으로 데이터 자체를 변경함  \n",
    "    \n",
    "3. **데이터정리**: 데이터한곳에담기(Data Warehouse) + 바꾸기및정리(Data Mart) + 분리(Data Split)\n",
    "    - 데이터수집/전처리/정리 까지 전체 업무의 80% 이상을 차지한다\n",
    "        > **1회성 수집/전저리/정리로 끝나지 않고 끊임없이 업데이트하고 진화시켜야 한다(이는 분석알고리즘이 해주지 않는다)**  \n",
    "        > - Data Warehouse 목적: 전처리 단계를 거친 1개의 Database를 주로 보관 및 무결점 유지 목적\n",
    "        > - Data Mart 목적: Warehouse를 변경하지 않고 복사하여 조금 더 목적에 맞게 전처리를 거침\n",
    "        > - Data Split 목적: 주로 과거(Train Data)와 미래(Test Data)를 구분하여 저장/알고리즘에 활용\n",
    "    \n",
    "4. **데이터분석**: 기초통계(Descriptive Statistics) + 모델링(Algorithm) + 검증(Evaluation) + 에러분석(Error Analysis)  \n",
    "    - 수학적으론 어려울 수 있지만 수동적으로 대응/활용이 가능하다\n",
    "    - 알고리즘(또는 기계)은 정해진 검증수단을 따를뿐 우리의 문제에 관심이 없다\n",
    "        > **각 알고리즘의 사용 목적에 대한 명확한 이해와 결과해석을 집중해서 습득해야 한다**  \n",
    "        > **어떤 알고리즘 성능 뛰어난지 검증(Evaluation)은 결국 사람이기에 많은 고민을 해야 한다**\n",
    "        > **알고리즘 적용시작이 중요한게 아니라 언제 끝내야 하는지 고민해야 한다**\n",
    "        > - Descriptive Statistics 목적: 어떤 분석 알고리즘을 선정할지 또는 Input/Output 형태를 결정하는 기준으로 활용\n",
    "        > - Algorithm 목적: Input/Output의 형태 또는 분석목적에 따라 정해지는 편\n",
    "        > - Evaluation 목적: 현 알고리즘 성능 확인 및 다음 업데이트를 위한 기준 설정\n",
    "        > - Error Analysis 목적: 모든 데이터의 패턴/특징을 알고리즘이 반영하고 있음을 이해하기 위한 기준\n",
    "    \n",
    "5. **결과정리**: 시각화(Visualization/Dashboard) + 의사결정(Decision Support) + 지식화(Knowledge) + 공유(Reporting)\n",
    "    > **0~4 단계를 무한대로 반복 및 각 단계를 업데이트하며 인싸이트를 뽑아낼 수 있어야 한다**\n",
    "    > - Visualization/Dashboard/Decision/Knowledge/Reporting 목적: 주로 고객에 맞춘 설명력을 제공하기 위함으로 일반화된 방향은 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 준비\n",
    "\n",
    "* 분석을 통해서 의미를 도출해낼만한 데이터인지?\n",
    "* 결과로 도출되는 결과를 충분하게 설명할 수 있는지?\n",
    "* 기록하고 있는 정보가 유의미한지 생각해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 보관\n",
    "\n",
    "* 분석에 적절한 크기/위치에 데이터가 적재되어 있는지?\n",
    "* 이곳저곳에 흩어져있는 정보들을 분석하기 용이하게 분류/적재해놓자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로딩\n",
    " * 데이터의 행/열의 개수\n",
    " * 데이터 행의 타입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 기본분석\n",
    " * 데이터 디스크립션\n",
    " * 기초 통계량\n",
    " * 수량형/범주형\n",
    " * 결측값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 시각화\n",
    "\n",
    "* 한 수량형 변수\n",
    "* 한 범주형 변수\n",
    "* 두 수량형 변수\n",
    "* 수량형변수와 범주형변수\n",
    "* 두 범주형 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 문제정의\n",
    "\n",
    "* 문제정의\n",
    "> 데이터로 풀수있는 문제인지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분석계획\n",
    "\n",
    "* 어느 카테고리의 문제인가?\n",
    "* 일반적인 머신러닝? 시계열특성? 추천? 비지도학습이나 강화학습으로 더 잘풀릴수 있는지?\n",
    "* 베이스라인 알고리즘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 알고리즘\n",
    "https://en.wikipedia.org/wiki/Outline_of_machine_learning\n",
    "1. Supervised Learning\n",
    "    1-1. Regression Algorithms\n",
    "        - Ordinary Least Squares Regression(OLSR)\n",
    "        - Linear Regression\n",
    "        - Logistic Regression\n",
    "        - Stepwise Regression\n",
    "        - Multivariate Adaptive Regression Splines(MARS)\n",
    "        - Locally Estimated Scatterplot Smoothing(LOESS)\n",
    "    1-2. Instance-based Algorithms\n",
    "        - K-Nearest Neighbor(KNN)\n",
    "        - Learning Vector Quantization(LVQ)\n",
    "        - Self-Organizing Map(SOM)\n",
    "        - Locally Weighted Learning(LWN)\n",
    "    1-3. Regularization Algorithms\n",
    "        - Ridge Regression\n",
    "        - Least Absolute Shrinkage and Selection Operator(LASSO)\n",
    "        - Elastic Net(Ridge + LASSO)\n",
    "        - Least-Angle Regression(LARS)\n",
    "    1-4. Decision Tree Algorithms\n",
    "        - Classification and Regression Tree(CART)\n",
    "        - Iterative Dichotomiser3 (ID3)\n",
    "        - C4.5 and C5.0\n",
    "        - Chi-squared Automatic Interaction Detection (CHAID)\n",
    "        - Decision Stump\n",
    "        - M5\n",
    "        - Conditional Decition Trees\n",
    "    1-5. Bayesian Algorithms\n",
    "        - Naive Bayes\n",
    "        - Gaussian Naive Bayes\n",
    "        - Multinomial Naive Bayes\n",
    "        - Averaged One-Dependence Estimators (AODE)\n",
    "        - Bayesian Belief Network (BBN)\n",
    "        - Bayesian Network (BN)\n",
    "    1-6. Artificial Neural Network Algorithms\n",
    "        - Perceptron\n",
    "        - Back-Propagation\n",
    "        - Hopfield Network\n",
    "        - Radial Basis Function Network (RBFN)\n",
    "\n",
    "2. Unsupervised Learning\n",
    "    2-1. Clustering Algorithms\n",
    "        - K-Means\n",
    "        - K-Medians\n",
    "        - Expectation Maximisation (EM)\n",
    "        - Hierarchical Clustering\n",
    "    2-2. Association Rule Learning Algorithms\n",
    "        - Apriori Algorithms\n",
    "        - Eclat Algorithms\n",
    "    2-3. Dimensionality Reduction Algorithms\n",
    "        - Principal Component Analysis (PCA)\n",
    "        - Principal Component Regression (PCR)\n",
    "        - Partial Least Squares Regression (PLSR)\n",
    "        - Sammon Mapping\n",
    "        - Multidimensional Scaling (MDS)\n",
    "        - Projection Pursuit\n",
    "        - Linear Discriminant Analysis (LDA)\n",
    "        - Mixture Discriminant Analysis (MDA)\n",
    "        - Quadratic Discriminant Analysis (QDA)\n",
    "        - Flexible Discriminant Analysis (FDA)\n",
    "    2-4. Ensemble Algorithms\n",
    "        - Boosting\n",
    "        - Bootstrapped Aggregation (Bagging)\n",
    "        - AdaBoost\n",
    "        - Stacked Generalization (Blending)\n",
    "        - Gradient Boosting Machines (GBM)\n",
    "        - Gradient Boosted Regression Trees (GBRT)\n",
    "        - Random Forest\n",
    "    2-5. Deep Learning Algorithms\n",
    "        - Deep Boltzmann Machine (DBM)\n",
    "        - Deep Belief Networks (DBN)\n",
    "        - Convolutional Neural Network (CNN)\n",
    "        - Recorrent Neural Network (RNN)\n",
    "        - Stacked AutoEncoders\n",
    "\n",
    "3. Reinforcement Learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련/테스트 데이터분류\n",
    "\n",
    "* 일반적인 경우라면 train/valid/test를 비율로 조절\n",
    "* 시계열의 경우라면 과거2년~과거1년/과거1년~현재등의 시간 비율로 조절\n",
    "* 아니면 둘의 특성을 전부고려해서 조절\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수간 스케일 차이 조정(Scaling)\n",
    "\n",
    "- **목적:**\n",
    "    - **(하드웨어)**  \n",
    "    : PC 메모리를 고려하여 오버플로우(Overflow)나 언더플로우(Underflow)를 방지 \n",
    "    - **(소프트웨어)**  \n",
    "    : 독립 변수의 공분산 행렬 조건수(Condition Number)를 감소시켜 최적화 안정성 및 수렴 속도 향상 \n",
    "\n",
    "**1) Standard Scaler:** <center>$\\dfrac{X_{it} - E(X_i)}{SD(X_i)}$</center>\n",
    "> 기본 스케일로 평균을 제외하고 표준편차를 나누어 변환  \n",
    "> 각 변수(Feature)가 정규분포를 따른다는 가정이기에 정규분포가 아닐 시 최선이 아닐 수 있음  \n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.StandardScaler().fit()\n",
    "sklearn.preprocessing.StandardScaler().transform()\n",
    "sklearn.preprocessing.StandardScaler().fit_transform()\n",
    "~~~\n",
    "\n",
    "**2) Min-Max Scaler:** <center>$\\dfrac{X_{it} - min(X_i)}{max(X_i) - min(X_i)}$</center>\n",
    "> 가장 많이 활용되는 알고리즘으로 최소\\~최대 값이 0\\~1 또는 -1\\~1 사이의 값으로 변환  \n",
    "> 각 변수(Feature)가 정규분포가 아니거나 표준편차가 매우 작을 때 효과적  \n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.MinMaxScaler().fit()\n",
    "sklearn.preprocessing.MinMaxScaler().transform()\n",
    "sklearn.preprocessing.MinMaxScaler().fit_transform()\n",
    "~~~\n",
    "\n",
    "**3) Robust Scaler:** <center>$\\dfrac{X_{it} - Q_1(X_i)}{Q_3(X_i) - Q_1(X_i)}$</center>\n",
    "> 최소-최대 스케일러와 유사하지만 최소/최대 대신에 IQR(Interquartile Range) 중 25%값/75%값을 사용하여 변환  \n",
    "> 이상치(Outlier)에 영향을 최소화하였기에 이상치가 있는 데이터에 효과적이고 적은 데이터에도 효과적인 편  \n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.RobustScaler().fit()\n",
    "sklearn.preprocessing.RobustScaler().transform()\n",
    "sklearn.preprocessing.RobustScaler().fit_transform()\n",
    "~~~\n",
    "\n",
    "**4) Normalizer:** <center>$\\dfrac{X_{it}}{\\sqrt{X_{i}^2 + X_{j}^2 + ... + X_{k}^2}}$</center>\n",
    "> 각 변수(Feature)를 전체 $n$개 모든 변수들의 크기들로 나누어서 변환(by Cartesian Coordinates)  \n",
    "> 각 변수들의 값은 원점으로부터 반지름 1만큼 떨어진 범위 내로 변환  \n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.Normalizer().fit()\n",
    "sklearn.preprocessing.Normalizer().transform()\n",
    "sklearn.preprocessing.Normalizer().fit_transform()\n",
    "~~~\n",
    "\n",
    "- [**비교 예시**](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-download-auto-examples-preprocessing-plot-all-scaling-py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성화함수\n",
    "\n",
    "최적화함수\n",
    "\n",
    "성능측정함수\n",
    "\n",
    "검증지표\n",
    "\n",
    "잔차진단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터준비\n",
    "\n",
    "* 데이터 결측\n",
    "* 데이터 스케일링\n",
    "* 변수중요도(중요변수 선택)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이프라인 구성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "182px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
